{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#data = pd.read_json('../input/Cell_Phones_and_Accessories_5.json', lines=True)\n",
    "data = pd.read_csv('data/amazon-reviews-sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ad86f27-7e53-4a9a-ba4c-4c1b39f0b40d",
    "_uuid": "ff1e996842ab005dd5c37d8590ceaf9466f5e6c7"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89ebf1cb-79fb-4b34-a997-fe5a420b8961",
    "_uuid": "ee7c899df02972a1aa1148af40d8c48171d612a0"
   },
   "outputs": [],
   "source": [
    "#Data Exploration\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7922f602-25af-4236-95b9-cad8959e6998",
    "_uuid": "170807cd28cec2c02ccaeac483d8189649e4b723"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6b9a7cf-5f71-4dca-a490-a8226193aa31",
    "_uuid": "338e8f9825d0c92cb89ea7895d314dd34b80bdb7"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c9a89ba-8558-4a0b-8ba0-2b11e21dff26",
    "_uuid": "157503072c46670e9af87a8a9969e11de163a7e8"
   },
   "source": [
    "# Top 10 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ecd8f5e-273f-43c0-b54b-86a76c0e7619",
    "_uuid": "5c721209582fc0acb380b20f44735ada5e418726"
   },
   "outputs": [],
   "source": [
    "rating = data.groupby('title').describe()['overall'].sort_values('mean', ascending=False)[['mean']]\n",
    "rating.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "960ae3d9-f172-48fc-9ccc-a038212437ea",
    "_uuid": "70370292336295c0fb202ccf5f07181f4328fe21"
   },
   "source": [
    "# Most Popular Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ae97e2c5-dd0b-4efb-b6ee-b6f998caae68",
    "_uuid": "3ef22b66a49419197dd231defc890acb22aa84f1"
   },
   "outputs": [],
   "source": [
    "# find the 10 most frequent product_type_names.\n",
    "from collections import Counter\n",
    "product_type_count = Counter(list(data['title']))\n",
    "product_type_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aae89f94-b71b-42a4-9f19-a32c6382d361",
    "_uuid": "d605008762fa433e00d1baac2f384bf59021daff"
   },
   "source": [
    "# Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8077f4a3-696d-4931-8778-2e1d84a94a0e",
    "_uuid": "a546afb6c38d1a28c9916b13cc2203e965572228"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be094ad5-b143-4a27-8d3b-ffdd75b60f04",
    "_uuid": "467f6e3851517fd791206017c4d33c530ec12d4c"
   },
   "outputs": [],
   "source": [
    "# Sort the whole data based on title (alphabetical order of title) \n",
    "data_copy = data.copy()\n",
    "data_sorted = data.sort_values('title', ascending=False)\n",
    "data_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd57ba06-bb8a-4377-89e0-3b722d68c64b",
    "_uuid": "63687b4b9a50c1736c61675d1661e5914f3c7274"
   },
   "outputs": [],
   "source": [
    "data_unique = data_sorted.drop_duplicates(subset='asin', keep=\"last\")\n",
    "data_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fa0fd0c4-d4ab-45bd-824c-4c358429a72b",
    "_uuid": "78830d70dab25072b5b270a749eb3aa7fce77278"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print ('list of stop words:', list(stop_words)[:10])\n",
    "\n",
    "def nlp_preprocessing(total_text, index, column):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        for words in total_text.split():\n",
    "            # remove the special chars in review like '\"#$@!%^&*()_+-~?>< etc.import time\n",
    "            word = (\"\".join(e for e in words if e.isalnum()))\n",
    "            # Conver all letters to lower-case\n",
    "            word = word.lower()\n",
    "            # stop-word removal\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        data_unique[column][index] = string\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1a339576-e48b-4691-9b2f-1125174026e3",
    "_uuid": "040d2a61df058d695a1825177b03fb9d9693d225"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.clock()\n",
    "# we take each title and we text-preprocess it.\n",
    "for index, row in data_unique.iterrows():\n",
    "    nlp_preprocessing(row['title'], index, 'title')\n",
    "# we print the time it took to preprocess whole titles \n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff217798-0ad8-4f9f-8e7f-9466011ee60a",
    "_uuid": "07f31fed4065b5567a509ff32c461dcb841efd44"
   },
   "outputs": [],
   "source": [
    "data_unique.to_csv(\"output/preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f4459cc9-b17c-4ed6-9de1-bdfc7a6a4cc7",
    "_uuid": "5139e5d5998d42b41fda831eab9b66c7439fb81f"
   },
   "outputs": [],
   "source": [
    "data_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb796f6b-d6f5-4c6e-91d4-dfd12709573f",
    "_uuid": "aa35256fe80b5a70e7f86608628a7a534997d41d"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd05817a-fcb0-4d78-b318-72545991d172",
    "_uuid": "7a5d2b91b5a4b71ff476500633690364c0a9dbd2"
   },
   "outputs": [],
   "source": [
    "#plotting code to understand the algorithm's decision.\n",
    "def plot_heatmap(keys, values, labels, text):\n",
    "        # keys: list of words of recommended title\n",
    "        # values: len(values) ==  len(keys), values(i) represents the occurence of the word keys(i)\n",
    "        # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "                # if model == 'bag of words': labels(i) = values(i)\n",
    "                # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "                # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "        # url : apparel's url\n",
    "\n",
    "        # we will devide the whole figure into two parts\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[4,1], height_ratios=[4,1]) \n",
    "        fig = plt.figure(figsize=(25,3))\n",
    "        \n",
    "        # 1st, ploting heat map that represents the count of commonly ocurred words in title2\n",
    "        ax = plt.subplot(gs[0])\n",
    "        # it displays a cell in white color if the word is intersection(lis of words of title1 and list of words of title2), in black if not\n",
    "        ax = sns.heatmap(np.array([values]), annot=np.array([labels]))\n",
    "        ax.set_xticklabels(keys) # set that axis labels as the words of title\n",
    "        ax.set_title(text) # apparel title\n",
    "        \n",
    "        \n",
    "        # displays combine figure ( heat map and image together)\n",
    "        plt.show()\n",
    "        \n",
    "def plot_heatmap_image(doc_id, vec1, vec2, text, model):\n",
    "\n",
    "    # doc_id : index of the title1\n",
    "    # vec1 : input apparels's vector, it is of a dict type {word:count}\n",
    "    # vec2 : recommended apparels's vector, it is of a dict type {word:count}\n",
    "    # url : apparels image url\n",
    "    # text: title of recomonded apparel (used to keep title of image)\n",
    "    # model, it can be any of the models, \n",
    "        # 1. bag_of_words\n",
    "        # 2. tfidf\n",
    "        # 3. idf\n",
    "\n",
    "    # we find the common words in both titles, because these only words contribute to the distance between two title vec's\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys()) \n",
    "\n",
    "    # we set the values of non intersecting words to zero, this is just to show the difference in heatmap\n",
    "    for i in vec2:\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "\n",
    "    # for labeling heatmap, keys contains list of all words in title2\n",
    "    keys = list(vec2.keys())\n",
    "    #  if ith word in intersection(lis of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "        # if model == 'bag of words': labels(i) = values(i)\n",
    "        # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "        # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "\n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    elif model == 'tfidf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # tfidf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # tfidf_title_features[doc_id, index_of_word_in_corpus] will give the tfidf value of word in given document (doc_id)\n",
    "            if x in  tfidf_title_vectorizer.vocabulary_:\n",
    "                labels.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # idf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # idf_title_features[doc_id, index_of_word_in_corpus] will give the idf value of word in given document (doc_id)\n",
    "            if x in  idf_title_vectorizer.vocabulary_:\n",
    "                labels.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "    plot_heatmap(keys, values, labels, text)\n",
    "\n",
    "        \n",
    "# this function gets a list of wrods along with the frequency of each \n",
    "# word given \"text\"\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    # words stores list of all words in given string, you can try 'words = text.split()' this will also gives same result\n",
    "    return Counter(words) # Counter counts the occurence of each word in list, it returns dict type object {word1:count}\n",
    "\n",
    "\n",
    "\n",
    "def get_result(doc_id, content_a, content_b, model):\n",
    "    text1 = content_a\n",
    "    text2 = content_b\n",
    "    \n",
    "    # vector1 = dict{word11:#count, word12:#count, etc.}\n",
    "    vector1 = text_to_vector(text1)\n",
    "\n",
    "    # vector1 = dict{word21:#count, word22:#count, etc.}\n",
    "    vector2 = text_to_vector(text2)\n",
    "\n",
    "    plot_heatmap_image(doc_id, vector1, vector2, text2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0d8fc4e-8018-4045-b958-ab258575b138",
    "_uuid": "8d802896634897a28afa08016f39f35a31ec16d6"
   },
   "outputs": [],
   "source": [
    "print(data_unique.shape)\n",
    "data_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea152044-07d2-46c2-9ae3-58f4bb1f070b",
    "_uuid": "7892545baef14176c17c4f1f45bd900105ef2b7a"
   },
   "outputs": [],
   "source": [
    "tfidf_title_vectorizer = TfidfVectorizer(min_df = 0)\n",
    "tfidf_title_features = tfidf_title_vectorizer.fit_transform(data_unique['title'])\n",
    "print(tfidf_title_features[:5])\n",
    "print(tfidf_title_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9f2cd72b-6626-459d-af33-947bda60dfeb",
    "_uuid": "47c221c16ffc30ef5552b5853995c0c5057b4fb2"
   },
   "outputs": [],
   "source": [
    "def tfidf_model(doc_id, num_results):\n",
    "    # doc_id: apparel's id in given corpus\n",
    "    \n",
    "    # pairwise_dist will store the distance from given input apparel to all remaining apparels\n",
    "    # the metric we used here is cosine, the coside distance is mesured as K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "    # http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    "    pairwise_dist = pairwise_distances(tfidf_title_features,tfidf_title_features[doc_id])\n",
    "\n",
    "    # np.argsort will return indices of 9 smallest distances\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results+5]\n",
    "    #pdists will store the 9 smallest distances\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results+5]\n",
    "\n",
    "    #data frame indices of the 9 smallest distace's\n",
    "    df_indices = list(data_unique.index[indices])\n",
    "\n",
    "    for i in range(0,len(indices)):\n",
    "        # we will pass 1. doc_id, 2. title1, 3. title2, url, model\n",
    "        if(pdists[i] != 0.0):\n",
    "            get_result(indices[i], data_unique['title'].loc[df_indices[0]], data_unique['title'].loc[df_indices[i]], 'tfidf')\n",
    "            print('ASIN :',data_unique['asin'].loc[df_indices[i]])\n",
    "            print ('Eucliden distance from the given image :', pdists[i])\n",
    "            print('='*125)\n",
    "tfidf_model(1, 5)\n",
    "# in the output heat map each value represents the tfidf values of the label word, the color represents the intersection with inputs title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2adae83c-1d24-4036-8b35-8195c32a76b4",
    "_uuid": "6758edca84aab957a5b3eb8f39f0ba5da4a8f423"
   },
   "outputs": [],
   "source": [
    "tfidf_model(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f530b296-844f-47ad-b5ab-a9194666680d",
    "_uuid": "61259fe8fb4eece3bfdaf407ba875b85d1a24d3c"
   },
   "outputs": [],
   "source": [
    "data_unique_copy = data_unique.copy().reset_index()\n",
    "print(data_unique_copy['title'].head(20))\n",
    "#ip = input(\"Enter your choice: \")\n",
    "#tfidf_model(ip, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
